{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411bbdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "print(\"Mixed precision enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18162654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Clinical-Longformer tokenizer...\n",
      "Tokenizer loaded. Vocabulary size: 50,265\n",
      "\n",
      "Train: 82,501\n",
      "Val: 9,084\n",
      "Test: 23,048\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Load Data and Tokenizer\n",
    "train_df = pd.read_csv('../data/Processed/train.csv')\n",
    "val_df = pd.read_csv('../data/Processed/val.csv')\n",
    "test_df = pd.read_csv('../data/Processed/test.csv')\n",
    "\n",
    "with open('../data/Processed/mlb.pkl', 'rb') as f:\n",
    "    mlb = pickle.load(f)\n",
    "with open('../data/Processed/top50_codes.pkl', 'rb') as f:\n",
    "    top50_codes = pickle.load(f)\n",
    "\n",
    "train_df['icd_codes'] = train_df['icd_codes'].apply(ast.literal_eval)\n",
    "val_df['icd_codes'] = val_df['icd_codes'].apply(ast.literal_eval)\n",
    "test_df['icd_codes'] = test_df['icd_codes'].apply(ast.literal_eval)\n",
    "\n",
    "print(\"Loading Clinical-Longformer tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "print(f\"Tokenizer loaded. Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da313056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 82,501\n",
      "Val: 9,084\n",
      "Test: 23,048\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — PLM-ICD Dataset with Full Document Processing\n",
    "class PLMICDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n",
    "\n",
    "y_train = mlb.transform(train_df['icd_codes'])\n",
    "y_val = mlb.transform(val_df['icd_codes'])\n",
    "y_test = mlb.transform(test_df['icd_codes'])\n",
    "\n",
    "train_dataset = PLMICDDataset(train_df['text_clean'].values, y_train, tokenizer, max_length=512)\n",
    "val_dataset = PLMICDDataset(val_df['text_clean'].values, y_val, tokenizer, max_length=512)\n",
    "test_dataset = PLMICDDataset(test_df['text_clean'].values, y_test, tokenizer, max_length=512)\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,}\")\n",
    "print(f\"Val: {len(val_dataset):,}\")\n",
    "print(f\"Test: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 5,157\n",
      "Val batches: 568\n",
      "Test batches: 1,441\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Collate Function and DataLoaders\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print(f\"Val batches: {len(val_loader):,}\")\n",
    "print(f\"Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f17d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 148,736,356\n",
      "Clinical-Longformer PLM-ICD model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — PLM-ICD with Clinical-Longformer Backbone\n",
    "class PLMICD(nn.Module):\n",
    "    def __init__(self, num_labels=50, dropout=0.1):\n",
    "        super(PLMICD, self).__init__()\n",
    "        self.longformer = AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.label_attention = nn.Linear(768, num_labels)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        token_output = outputs.last_hidden_state\n",
    "        token_output = self.dropout(token_output)\n",
    "\n",
    "        attention_scores = self.label_attention(token_output)\n",
    "\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "        attention_scores = attention_scores * attention_mask_expanded\n",
    "        attention_scores = attention_scores - (1 - attention_mask_expanded) * 1e9\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        label_representations = torch.bmm(\n",
    "            attention_weights.transpose(1, 2),\n",
    "            token_output\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(label_representations)\n",
    "        logits = torch.diagonal(logits, dim1=1, dim2=2)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = PLMICD(num_labels=50)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"Clinical-Longformer PLM-ICD model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0/5157 | Loss: 0.7005\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Training Loop with Mixed Precision\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_val_f1 = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5000 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            with autocast():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} complete | Avg Loss: {avg_loss:.4f} | Val Micro F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '../models/plmicd_longformer_best.pt')\n",
    "        print(f\"Best model saved with F1: {best_val_f1:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best Val F1: {best_val_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
