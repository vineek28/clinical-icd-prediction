{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ead133-061c-4fdb-88ac-1c208e5f8184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.17.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: fastparquet in /usr/local/lib/python3.11/dist-packages (2025.12.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (3.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastparquet) (1.26.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2024.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Device: cuda\n",
      "GPU: NVIDIA A40\n",
      "VRAM: 47.7 GB\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Install and Setup\n",
    "!pip install transformers accelerate -q\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install numpy==1.26.4 -q\n",
    "!pip install fastparquet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "scaler = GradScaler()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"Mixed precision enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08dbae0c-87fc-440b-9cd4-7dd266461553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Clinical-Longformer tokenizer...\n",
      "Tokenizer loaded. Vocabulary size: 50,265\n",
      "Train: 82,501\n",
      "Val: 9,084\n",
      "Test: 23,048\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Load Data and Tokenizer\n",
    "train_df = pd.read_parquet('/workspace/train.parquet')\n",
    "val_df = pd.read_parquet('/workspace/val.parquet')\n",
    "test_df = pd.read_parquet('/workspace/test.parquet')\n",
    "\n",
    "with open('/workspace/mlb.pkl', 'rb') as f:\n",
    "    mlb = pickle.load(f)\n",
    "with open('/workspace/top50_codes.pkl', 'rb') as f:\n",
    "    top50_codes = pickle.load(f)\n",
    "\n",
    "train_df['icd_codes'] = train_df['icd_codes'].apply(ast.literal_eval)\n",
    "val_df['icd_codes'] = val_df['icd_codes'].apply(ast.literal_eval)\n",
    "test_df['icd_codes'] = test_df['icd_codes'].apply(ast.literal_eval)\n",
    "\n",
    "print(\"Loading Clinical-Longformer tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "print(f\"Tokenizer loaded. Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d778e82-7143-4fbd-a06c-944446535d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 82,501\n",
      "Val: 9,084\n",
      "Test: 23,048\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Dataset\n",
    "class PLMICDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n",
    "\n",
    "y_train = mlb.transform(train_df['icd_codes'])\n",
    "y_val = mlb.transform(val_df['icd_codes'])\n",
    "y_test = mlb.transform(test_df['icd_codes'])\n",
    "\n",
    "train_dataset = PLMICDDataset(train_df['text_clean'].values, y_train, tokenizer, max_length=512)\n",
    "val_dataset = PLMICDDataset(val_df['text_clean'].values, y_val, tokenizer, max_length=512)\n",
    "test_dataset = PLMICDDataset(test_df['text_clean'].values, y_test, tokenizer, max_length=512)\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,}\")\n",
    "print(f\"Val: {len(val_dataset):,}\")\n",
    "print(f\"Test: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4a31a5-5b47-461e-bfaf-5a7ff4cab380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 10,313\n",
      "Val batches: 1,136\n",
      "Test batches: 2,881\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — DataLoaders\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print(f\"Val batches: {len(val_loader):,}\")\n",
    "print(f\"Test batches: {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb634f71-c0d3-4342-9ea3-cd6cf0faf79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d5c9876b5a4d1bbe1eb9ed0cf71c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLongformerModel LOAD REPORT\u001b[0m from: yikuan8/Clinical-Longformer\n",
      "Key                                | Status     | \n",
      "-----------------------------------+------------+-\n",
      "lm_head.bias                       | UNEXPECTED | \n",
      "lm_head.decoder.weight             | UNEXPECTED | \n",
      "longformer.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias                 | UNEXPECTED | \n",
      "lm_head.layer_norm.bias            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight          | UNEXPECTED | \n",
      "lm_head.dense.weight               | UNEXPECTED | \n",
      "lm_head.decoder.bias               | UNEXPECTED | \n",
      "pooler.dense.bias                  | MISSING    | \n",
      "pooler.dense.weight                | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 148,736,356\n",
      "PLM-ICD Clinical-Longformer model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — PLM-ICD Model with Clinical-Longformer + Label Attention\n",
    "class PLMICD(nn.Module):\n",
    "    def __init__(self, num_labels=50, dropout=0.1):\n",
    "        super(PLMICD, self).__init__()\n",
    "        self.longformer = AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.label_attention = nn.Linear(768, num_labels)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        token_output = outputs.last_hidden_state\n",
    "        token_output = self.dropout(token_output)\n",
    "\n",
    "        attention_scores = self.label_attention(token_output)\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "        attention_scores = attention_scores * attention_mask_expanded\n",
    "        attention_scores = attention_scores - (1 - attention_mask_expanded) * 1e9\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        label_representations = torch.bmm(\n",
    "            attention_weights.transpose(1, 2),\n",
    "            token_output\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(label_representations)\n",
    "        logits = torch.diagonal(logits, dim1=1, dim2=2)\n",
    "        return logits\n",
    "\n",
    "model = PLMICD(num_labels=50)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"PLM-ICD Clinical-Longformer model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "683fcee5-fa21-4410-a50d-87946af03dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0/10313 | Loss: 0.7122\n",
      "Epoch 1 | Batch 1000/10313 | Loss: 0.2991\n",
      "Epoch 1 | Batch 2000/10313 | Loss: 0.2574\n",
      "Epoch 1 | Batch 3000/10313 | Loss: 0.2364\n",
      "Epoch 1 | Batch 4000/10313 | Loss: 0.2663\n",
      "Epoch 1 | Batch 5000/10313 | Loss: 0.1605\n",
      "Epoch 1 | Batch 6000/10313 | Loss: 0.3431\n",
      "Epoch 1 | Batch 7000/10313 | Loss: 0.2587\n",
      "Epoch 1 | Batch 8000/10313 | Loss: 0.2966\n",
      "Epoch 1 | Batch 9000/10313 | Loss: 0.1876\n",
      "Epoch 1 | Batch 10000/10313 | Loss: 0.2449\n",
      "\n",
      "Epoch 1 complete | Avg Loss: 0.2561 | Val Micro F1: 0.4920\n",
      "Best model saved with F1: 0.4920\n",
      "\n",
      "Epoch 2 | Batch 0/10313 | Loss: 0.2077\n",
      "Epoch 2 | Batch 1000/10313 | Loss: 0.2735\n",
      "Epoch 2 | Batch 2000/10313 | Loss: 0.1982\n",
      "Epoch 2 | Batch 3000/10313 | Loss: 0.1513\n",
      "Epoch 2 | Batch 4000/10313 | Loss: 0.2073\n",
      "Epoch 2 | Batch 5000/10313 | Loss: 0.1675\n",
      "Epoch 2 | Batch 6000/10313 | Loss: 0.1637\n",
      "Epoch 2 | Batch 7000/10313 | Loss: 0.2140\n",
      "Epoch 2 | Batch 8000/10313 | Loss: 0.2471\n",
      "Epoch 2 | Batch 9000/10313 | Loss: 0.2099\n",
      "Epoch 2 | Batch 10000/10313 | Loss: 0.2606\n",
      "\n",
      "Epoch 2 complete | Avg Loss: 0.2136 | Val Micro F1: 0.5274\n",
      "Best model saved with F1: 0.5274\n",
      "\n",
      "Epoch 3 | Batch 0/10313 | Loss: 0.1853\n",
      "Epoch 3 | Batch 1000/10313 | Loss: 0.1648\n",
      "Epoch 3 | Batch 2000/10313 | Loss: 0.1493\n",
      "Epoch 3 | Batch 3000/10313 | Loss: 0.2592\n",
      "Epoch 3 | Batch 4000/10313 | Loss: 0.1941\n",
      "Epoch 3 | Batch 5000/10313 | Loss: 0.1517\n",
      "Epoch 3 | Batch 6000/10313 | Loss: 0.1900\n",
      "Epoch 3 | Batch 7000/10313 | Loss: 0.2507\n",
      "Epoch 3 | Batch 8000/10313 | Loss: 0.2840\n",
      "Epoch 3 | Batch 9000/10313 | Loss: 0.2265\n",
      "Epoch 3 | Batch 10000/10313 | Loss: 0.1609\n",
      "\n",
      "Epoch 3 complete | Avg Loss: 0.2014 | Val Micro F1: 0.5368\n",
      "Best model saved with F1: 0.5368\n",
      "\n",
      "\n",
      "Training complete. Best Val F1: 0.5368\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Training Loop with Mixed Precision\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_val_f1 = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            with autocast():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} complete | Avg Loss: {avg_loss:.4f} | Val Micro F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '/workspace/plmicd_best.pt')\n",
    "        print(f\"Best model saved with F1: {best_val_f1:.4f}\\n\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best Val F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841c0e9-7606-4967-acbc-074fde36b8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
